{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c30898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a674e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a948568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing: import dataset\n",
    "company1_train = pd.read_csv('Training Data/GS_Training.csv')\n",
    "company1_test = pd.read_csv('Test Data/GS_Testing.csv')\n",
    "company2_train = pd.read_csv('Training Data/JNJ_Training.csv')\n",
    "company2_test = pd.read_csv('Test Data/JNJ_Testing.csv')\n",
    "company3_train = pd.read_csv('Training Data/JPM_Training.csv')\n",
    "company3_test = pd.read_csv('Test Data/JPM_Testing.csv')\n",
    "company4_train = pd.read_csv('Training Data/NKE_Training.csv')\n",
    "company4_test = pd.read_csv('Test Data/NKE_Testing.csv')\n",
    "company5_train = pd.read_csv('Training Data/PFE_Training.csv')\n",
    "company5_test = pd.read_csv('Test Data/PFE_Testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61488aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating FIRST parameter (each day's high - low)\n",
    "\n",
    "def calculate_high_low_diff(df):\n",
    "    high_low_values = df[\"High\"] - df[\"Low\"]\n",
    "    new_df = pd.DataFrame({\"Date\": df[\"Date\"], \"High-Low\": high_low_values})\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b55cb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  High-Low\n",
      "0     2009-04-06  3.899994\n",
      "1     2009-04-07  6.190002\n",
      "2     2009-04-08  5.050003\n",
      "3     2009-04-09  7.530006\n",
      "4     2009-04-13  8.770004\n",
      "...          ...       ...\n",
      "2007  2017-03-27  6.220001\n",
      "2008  2017-03-28  4.649994\n",
      "2009  2017-03-29  2.449997\n",
      "2010  2017-03-30  2.619995\n",
      "2011  2017-03-31  2.070007\n",
      "\n",
      "[2012 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "goldman_high_minus_low = calculate_high_low_diff(company1_train)\n",
    "johnson_high_minus_low = calculate_high_low_diff(company2_train)\n",
    "jpmorgan_high_minus_low = calculate_high_low_diff(company3_train)\n",
    "nike_high_minus_low = calculate_high_low_diff(company4_train)\n",
    "pfizer_high_minus_low = calculate_high_low_diff(company5_train)\n",
    "\n",
    "print(goldman_high_minus_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d59692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating SECOND parameter (each day's close - open)\n",
    "\n",
    "def calculate_close_open_diff(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['Date'] = df['Date']\n",
    "    new_df['Close-Open'] = df['Close'] - df['Open']\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3c10ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Close-Open\n",
      "0     2009-04-06    0.650002\n",
      "1     2009-04-07    2.080002\n",
      "2     2009-04-08   -2.620003\n",
      "3     2009-04-09    4.540001\n",
      "4     2009-04-13    7.339996\n",
      "...          ...         ...\n",
      "2007  2017-03-27    2.179993\n",
      "2008  2017-03-28    3.750000\n",
      "2009  2017-03-29   -0.940002\n",
      "2010  2017-03-30    1.740005\n",
      "2011  2017-03-31   -0.809998\n",
      "\n",
      "[2012 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Variable 2\n",
    "# New DataFrame for each company's Close-Open Variable\n",
    "\n",
    "goldman_close_minus_open = calculate_close_open_diff(company1_train)\n",
    "johnson_close_minus_open = calculate_close_open_diff(company2_train)\n",
    "jpmorgan_close_minus_open = calculate_close_open_diff(company3_train)\n",
    "nike_close_minus_open = calculate_close_open_diff(company4_train)\n",
    "pfizer_close_minus_open = calculate_close_open_diff(company5_train)\n",
    "\n",
    "print(goldman_close_minus_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4aa24e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating 3rd|4th|5th parameters (7|14|21 Day Moving Average) and gets rid of NaN values\n",
    "\n",
    "def moving_average(dataframe, days):\n",
    "    # Create a new dataframe with the moving average\n",
    "    ma_df = pd.DataFrame()\n",
    "#     ma_df['Date'] = dataframe['Date']\n",
    "    ma_df['Moving Avg'] = dataframe['Close'].rolling(window=days).mean()\n",
    "\n",
    "    # Add a column with the date of each week\n",
    "    ma_df['Date'] = dataframe['Date']\n",
    "    \n",
    "#     # Group by week and get the last date and moving average for each week\n",
    "#     ma_df = ma_df.groupby('Week').tail(1)\n",
    "    \n",
    "    # Remove NaN values\n",
    "    ma_df = ma_df.dropna()\n",
    "\n",
    "    return ma_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ee38b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Moving Avg        Date\n",
      "6     119.751429  2009-04-15\n",
      "7     120.400000  2009-04-16\n",
      "8     121.045714  2009-04-17\n",
      "9     121.082857  2009-04-20\n",
      "10    120.515714  2009-04-21\n",
      "...          ...         ...\n",
      "2007  233.705715  2017-03-27\n",
      "2008  231.618572  2017-03-28\n",
      "2009  229.662857  2017-03-29\n",
      "2010  229.408572  2017-03-30\n",
      "2011  229.215714  2017-03-31\n",
      "\n",
      "[2006 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Variable 3\n",
    "# New DataFrame for each company's 7 day Moving Average\n",
    "\n",
    "goldman_7D_movingAverage = moving_average(company1_train, 7)\n",
    "johnson_7D_movingAverage = moving_average(company2_train, 7)\n",
    "jpmorgan_7D_movingAverage = moving_average(company3_train, 7)\n",
    "nike_7D_movingAverage = moving_average(company4_train, 7)\n",
    "pfizer_7D_movingAverage = moving_average(company5_train, 7)\n",
    "\n",
    "print(goldman_7D_movingAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d404b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Moving Avg        Date\n",
      "13    120.001429  2009-04-24\n",
      "14    120.305000  2009-04-27\n",
      "15    120.632857  2009-04-28\n",
      "16    121.520714  2009-04-29\n",
      "17    121.818571  2009-04-30\n",
      "...          ...         ...\n",
      "2007  241.115715  2017-03-27\n",
      "2008  239.622143  2017-03-28\n",
      "2009  238.070001  2017-03-29\n",
      "2010  236.844286  2017-03-30\n",
      "2011  235.527143  2017-03-31\n",
      "\n",
      "[1999 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Variable 4\n",
    "# New DataFrame for each company's 14 day Moving Average\n",
    "\n",
    "goldman_14D_movingAverage = moving_average(company1_train, 14)\n",
    "johnson_14D_movingAverage = moving_average(company2_train, 14)\n",
    "jpmorgan_14D_movingAverage = moving_average(company3_train, 14)\n",
    "nike_14D_movingAverage = moving_average(company4_train, 14)\n",
    "pfizer_14D_movingAverage = moving_average(company5_train, 14)\n",
    "\n",
    "print(goldman_14D_movingAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8468431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Moving Avg        Date\n",
      "20    122.557619  2009-05-05\n",
      "21    123.632381  2009-05-06\n",
      "22    124.472857  2009-05-07\n",
      "23    125.655714  2009-05-08\n",
      "24    126.200952  2009-05-11\n",
      "...          ...         ...\n",
      "2007  244.408572  2017-03-27\n",
      "2008  243.456191  2017-03-28\n",
      "2009  242.522381  2017-03-29\n",
      "2010  241.499047  2017-03-30\n",
      "2011  240.482857  2017-03-31\n",
      "\n",
      "[1992 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Variable 5\n",
    "# New DataFrame for each company's 21 day Moving Average\n",
    "\n",
    "goldman_21D_movingAverage = moving_average(company1_train, 21)\n",
    "johnson_21D_movingAverage = moving_average(company2_train, 21)\n",
    "jpmorgan_21D_movingAverage = moving_average(company3_train, 21)\n",
    "nike_21D_movingAverage = moving_average(company4_train, 21)\n",
    "pfizer_21D_movingAverage = moving_average(company5_train, 21)\n",
    "\n",
    "print(goldman_21D_movingAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d05ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating 6th parameter: Std for past 7 days and gets rid of NaN values\n",
    "\n",
    "def calculate_7day_std_dev(df):\n",
    "    # Create a new DataFrame with just the date and close columns\n",
    "    close_df = df[['Date', 'Close']].copy()\n",
    "\n",
    "    # Calculate the 7-day rolling standard deviation of the closing price\n",
    "    close_df['7 Day Std Dev'] = close_df['Close'].rolling(window=7).std()\n",
    "\n",
    "    # Drop the first 6 rows (since we don't have enough data to calculate the 7-day std dev for them)\n",
    "    close_df = close_df.dropna()\n",
    "\n",
    "    return close_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a548e8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date       Close  7 Day Std Dev\n",
      "6     2009-04-15  121.190002       5.785302\n",
      "7     2009-04-16  121.190002       5.632117\n",
      "8     2009-04-17  120.599998       5.303827\n",
      "9     2009-04-20  115.010002       5.253056\n",
      "10    2009-04-21  120.360001       5.054612\n",
      "...          ...         ...            ...\n",
      "2007  2017-03-27  225.479996       6.859184\n",
      "2008  2017-03-28  229.330002       5.263134\n",
      "2009  2017-03-29  228.449997       2.541953\n",
      "2010  2017-03-30  231.220001       2.221288\n",
      "2011  2017-03-31  229.720001       2.108751\n",
      "\n",
      "[2006 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Variable 6\n",
    "# New DataFrame for each company's Past 7 day std\n",
    "\n",
    "goldman_7D_STD = calculate_7day_std_dev(company1_train)\n",
    "johnson_7D_STD = calculate_7day_std_dev(company2_train)\n",
    "jpmorgan_7D_STD = calculate_7day_std_dev(company3_train)\n",
    "nike_7D_STD = calculate_7day_std_dev(company4_train)\n",
    "pfizer_7D_STD = calculate_7day_std_dev(company5_train)\n",
    "\n",
    "print(goldman_7D_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59b6182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(df1, df2, df3, df4, df5, df6, on_column):\n",
    "    # Merge the first two dataframes\n",
    "    merged_df = pd.merge(df1, df2, on=on_column)\n",
    "\n",
    "    # Merge the remaining dataframes\n",
    "    merged_df = pd.merge(merged_df, df3, on=on_column)\n",
    "    merged_df = pd.merge(merged_df, df4, on=on_column)\n",
    "    merged_df = pd.merge(merged_df, df5, on=on_column)\n",
    "    merged_df = pd.merge(merged_df, df6, on=on_column)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75587da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "company1_InputVars = merge_dataframes(goldman_high_minus_low, goldman_close_minus_open, goldman_7D_movingAverage, goldman_14D_movingAverage, goldman_21D_movingAverage, goldman_7D_STD, 'Date')\n",
    "company2_InputVars = merge_dataframes(johnson_high_minus_low, johnson_close_minus_open, johnson_7D_movingAverage, johnson_14D_movingAverage, johnson_21D_movingAverage, johnson_7D_STD, 'Date')\n",
    "company3_InputVars = merge_dataframes(jpmorgan_high_minus_low, jpmorgan_close_minus_open, jpmorgan_7D_movingAverage, jpmorgan_14D_movingAverage, jpmorgan_21D_movingAverage, jpmorgan_7D_STD, 'Date')\n",
    "company4_InputVars = merge_dataframes(nike_high_minus_low, nike_close_minus_open, nike_7D_movingAverage, nike_14D_movingAverage, nike_21D_movingAverage, nike_7D_STD, 'Date')\n",
    "company5_InputVars = merge_dataframes(pfizer_high_minus_low, pfizer_close_minus_open, pfizer_7D_movingAverage, pfizer_14D_movingAverage, pfizer_21D_movingAverage, pfizer_7D_STD, 'Date')\n",
    "\n",
    "print(company5_InputVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7078bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a501750",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['Goldman Sachs', 'Johnson & Johnson', 'JP Morgan and Co.', 'Nike', 'Pfizer Inc.']\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8847ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the ANN Model\n",
    "#Creating sequential object to define the layers\n",
    "classifier = Sequential()\n",
    "\n",
    "#Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=6))\n",
    "\n",
    "#Adding the second hidden layer\n",
    "classifier.add(Dense(units=4, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "#Adding the third hidden layer\n",
    "classifier.add(Dense(units=2, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "#Adding the output layer\n",
    "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='linear'))\n",
    "\n",
    "#Compiling the ANN\n",
    "classifier.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e8c30100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step\n",
      "63/63 [==============================] - 0s 3ms/step\n",
      "63/63 [==============================] - 0s 1ms/step\n",
      "63/63 [==============================] - 0s 1ms/step\n",
      "63/63 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#Iterate over each company's data and train and test model\n",
    "for i in range(1, 6):\n",
    "    #Selecting training and testing data for current company\n",
    "    X_train = globals()[f'company{i}_InputVars'].iloc[:, 2:].values\n",
    "    y_train = globals()[f'company{i}_InputVars'].iloc[:, 1].values\n",
    "    X_test = globals()[f'company{i}_InputVars'].iloc[:, 2:].values\n",
    "    y_test = globals()[f'company{i}_InputVars'].iloc[:, 1].values\n",
    "\n",
    "    #Scaling the input data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    #Fitting the ANN to the training set\n",
    "    classifier.fit(X_train_scaled, y_train, batch_size=32, epochs=100, verbose=0)\n",
    "\n",
    "    #Predicting the test set results\n",
    "    y_pred = classifier.predict(X_test_scaled)\n",
    "    y_pred = y_pred.reshape(-1) #reshape to 1D array\n",
    "\n",
    "\n",
    "    #Calculating the evaluation metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    mbe = np.mean(y_pred - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f33285d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pfizer Inc.: 0.14 22.32% -0.0132\n"
     ]
    }
   ],
   "source": [
    "#Printing the values for each company in the format: Company: RMSE MAPE MBE\n",
    "print(f\"{companies[i-1]}: {rmse:.2f} {mape:.2f}% {mbe:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30637f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
